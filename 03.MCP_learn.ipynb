{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9067dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create server parameters for stdio connection\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "from IPython.display import Markdown\n",
    "import os\n",
    "\n",
    "# Or load from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import (\n",
    "    SummarizationMiddleware,\n",
    "    TodoListMiddleware,\n",
    "    ShellToolMiddleware,\n",
    "    ToolRetryMiddleware,\n",
    ")\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain.messages import ToolMessage\n",
    "from utils.md_to_pdf import markdown_folder_to_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba8f5d",
   "metadata": {},
   "source": [
    "### One Server MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93e4700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the main, recent developments (late November 2025) in India–US relations:\n",
       "\n",
       "- Arms sale approved (20 Nov 2025)\n",
       "  - The US cleared a roughly $93 million package including FGM‑148 Javelin anti‑tank missiles and M982A1 Excalibur precision artillery rounds. The State Department/DSCA said the sales strengthen the strategic relationship and India can absorb the equipment. (BBC, multiple Indian outlets)\n",
       "\n",
       "- Trade negotiations advancing; aim to seal first tranche by year‑end\n",
       "  - India and the US have been holding regular virtual trade talks. India’s commerce secretary Rajesh Agarwal said most issues are resolved and the first tranche of a Bilateral Trade Agreement (and a parallel tariff framework) is expected to be locked before the end of 2025. Negotiations remain two‑track: a broader BTA and a focused deal to address reciprocal tariffs. (Economic Times, NDTV, Reuters coverage)\n",
       "\n",
       "- Tariff/tension background\n",
       "  - Earlier in 2025 the US imposed steep “reciprocal” and Russia‑related penalties (together reaching about 50% on many Indian exports). Those tariffs are a key item in the current talks; India seeks duty concessions for labour‑intensive sectors (textiles, gems, leather, seafood, etc.) while the US seeks concessions in certain industrial, auto (EV), and agricultural items.\n",
       "\n",
       "- Strategic/defence ties and broader context\n",
       "  - Defence cooperation has been growing (including recent framework agreements to expand cooperation). India continues to diversify away from Russian suppliers toward more US procurement, even as Russia remains a major source. The arms sale and talks reflect a strengthening but pragmatic strategic partnership amid trade frictions.\n",
       "\n",
       "- Other signals\n",
       "  - India’s regional clout is rising (Asia Power Index 2025 ranks India high), and both governments appear motivated to resolve economic disputes while deepening defence and strategic cooperation.\n",
       "\n",
       "If you want, I can:\n",
       "- Pull and link the latest full news articles (BBC, Economic Times, NDTV, Business Standard) for reading;\n",
       "- Summarize the trade deal positions in more detail (what each side wants and likely sticking points);\n",
       "- Create a short timeline of key events in 2025 (tariff moves, meetings, agreements, arms sales). Which would you prefer?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your math_server.py file\n",
    "    args=[\"/home/nitish/Documents/github/Langchain/tools/mcp_search_server.py\"],\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # Initialize the connection\n",
    "        await session.initialize()\n",
    "\n",
    "        # Get tools\n",
    "        tools = await load_mcp_tools(session)\n",
    "\n",
    "        # Create and run the agent\n",
    "        agent = create_agent(\"openai:gpt-5-mini\", tools)\n",
    "        agent_response = await agent.ainvoke({\"messages\": \"what's the latest news on India US relations?\"})\n",
    "\n",
    "Markdown(agent_response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d01e22",
   "metadata": {},
   "source": [
    "### Multi-Server MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d636a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "from pathlib import Path\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_filesystem.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"search\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_search_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# Configure middleware for the agent\n",
    "middleware = [\n",
    "    SummarizationMiddleware(\"groq:openai/gpt-oss-20b\", max_tokens_before_summary = 40000),\n",
    "    TodoListMiddleware(),\n",
    "    ShellToolMiddleware(\n",
    "        workspace_root=\"/home/nitish/Documents/github/Langchain\",\n",
    "        startup_commands=None,\n",
    "        shutdown_commands=None,\n",
    "    ),\n",
    "    ToolRetryMiddleware(\n",
    "        max_retries=3,  # Maximum number of retry attempts\n",
    "        backoff_factor=1.0,  # Exponential backoff factor between retries\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create and run the agent using Google AI (Gemini) - free for personal use\n",
    "#agent = create_agent(\"google_genai:models/gemini-flash-latest\", tools, middleware=middleware)\n",
    "#agent = create_agent(\"openai:gpt-5-mini\", tools, middleware=middleware)\n",
    "agent = create_agent(\"anthropic:claude-haiku-4-5-20251001\", tools, middleware=middleware)\n",
    "\n",
    "#agent = create_agent(\"groq:openai/gpt-oss-120b\", tools, middleware=middleware)\n",
    "#agent = create_agent(\"ollama:hf.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF:Q4_K_M\", tools, middleware=middleware)\n",
    "\n",
    "response = await agent.ainvoke({\"messages\": \"\"\"Define Autoencoders - use wiki. But mention in details. \n",
    "                                Then, prepare a 3 chapter course on them. Chapter 1 describes the basics and the mechanism. Feel free to use mermaid diagrams if necessary. \n",
    "                                The next chapter shows applications and the appropriate type of autoencoder. \n",
    "                                The last chapter shows tensorflow code for 3 kinds of autoencoders.\n",
    "                                write them in separate markdown files - in Autoencoders_Course folder.\"\"\"})\n",
    "Markdown(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c0b5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\nPrevious conversation was too long to summarize.', additional_kwargs={}, response_metadata={}, id='76d8076a-fe03-4725-b6cf-18dba8f71763'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 780, 'prompt_tokens': 10095, 'total_tokens': 10875, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 576, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 4736}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChEz9itR7wbsSwLskIHtr5WmIoWy3', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--a8e1a1fb-358c-4674-8047-f3998fbfb574-0', tool_calls=[{'name': 'write_todos', 'args': {'todos': [{'content': \"Fetch Wikipedia article on 'Autoencoder' and extract detailed information\", 'status': 'completed'}, {'content': \"Create folder 'Autoencoders_Course'\", 'status': 'in_progress'}, {'content': \"Write 'Autoencoders_Course/Definition.md' with detailed, wiki-based definition and citation\", 'status': 'pending'}, {'content': \"Write 'Autoencoders_Course/Chapter1_Basics.md' — basic dense autoencoder in TensorFlow (code + explanation)\", 'status': 'pending'}, {'content': \"Write 'Autoencoders_Course/Chapter2_Convolutional_Architectures.md' — convolutional autoencoders and practical tips (code + explanation)\", 'status': 'pending'}, {'content': \"Write 'Autoencoders_Course/Chapter3_Denoising_and_VAE.md' — denoising autoencoders and variational autoencoders (code + explanation)\", 'status': 'pending'}]}, 'id': 'call_jTthIp2uf0eE6xOAeRiozUlC', 'type': 'tool_call'}], usage_metadata={'input_tokens': 10095, 'output_tokens': 780, 'total_tokens': 10875, 'input_token_details': {'audio': 0, 'cache_read': 4736}, 'output_token_details': {'audio': 0, 'reasoning': 576}}),\n",
       "  ToolMessage(content='Updated todo list to [{\\'content\\': \"Fetch Wikipedia article on \\'Autoencoder\\' and extract detailed information\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Create folder \\'Autoencoders_Course\\'\", \\'status\\': \\'in_progress\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Definition.md\\' with detailed, wiki-based definition and citation\", \\'status\\': \\'pending\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter1_Basics.md\\' — basic dense autoencoder in TensorFlow (code + explanation)\", \\'status\\': \\'pending\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter2_Convolutional_Architectures.md\\' — convolutional autoencoders and practical tips (code + explanation)\", \\'status\\': \\'pending\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter3_Denoising_and_VAE.md\\' — denoising autoencoders and variational autoencoders (code + explanation)\", \\'status\\': \\'pending\\'}]', name='write_todos', id='3bd925c9-b749-4a90-9381-895c88af8094', tool_call_id='call_jTthIp2uf0eE6xOAeRiozUlC'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 10510, 'total_tokens': 10535, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 9728}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChEzKHYA6ULhHBswM3L6fjBQM6pyE', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--c559f9e3-276b-4a40-8aec-cd32b6c76e07-0', tool_calls=[{'name': 'create_directory', 'args': {'path': 'Autoencoders_Course', 'parents': True}, 'id': 'call_oWHfEeg2i4HV9kBjXEWt9ulk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 10510, 'output_tokens': 25, 'total_tokens': 10535, 'input_token_details': {'audio': 0, 'cache_read': 9728}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='{\\n  \"path\": \"Autoencoders_Course\",\\n  \"status\": \"already_exists\",\\n  \"message\": \"Directory already exists.\"\\n}', name='create_directory', id='6b36d91a-6faa-43a6-990b-ab969f0f4242', tool_call_id='call_oWHfEeg2i4HV9kBjXEWt9ulk'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 397, 'prompt_tokens': 10576, 'total_tokens': 10973, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 10496}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChEzO3S39M5T5OY93Y9av1VAhuBhk', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--b98bb521-7ace-4eac-90f9-868dd6c20c15-0', tool_calls=[{'name': 'write_file', 'args': {'path': 'Autoencoders_Course/Definition.md', 'content': '# Autoencoders — Definition (based on Wikipedia)\\n\\nAn autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). It learns two functions: an encoder that maps input data to a (usually lower-dimensional) latent representation (code), and a decoder that reconstructs the input from that code. The training objective is to minimize a reconstruction loss that measures the difference between the input and its reconstruction (commonly mean squared error or binary crossentropy).\\n\\nKey points from the Wikipedia article:\\n\\n- Purpose: efficient representation learning and dimensionality reduction; can produce lower-dimensional embeddings for downstream tasks.\\n- Structure: encoder (E) and decoder (D) families of functions, often implemented as multilayer perceptrons (MLPs) or deeper architectures.\\n- Undercomplete vs overcomplete: undercomplete autoencoders have a latent space dimension smaller than the input and tend to learn compressed representations; overcomplete autoencoders can learn identity mapping unless regularized, but may still learn useful features.\\n- Variants: sparse, denoising, contractive, variational (VAE), concrete, minimum description length (MDL-AE), k-sparse, etc. These introduce regularization or probabilistic formulations to encourage desirable properties in the learned representation.\\n- Training: typically via gradient-based optimization on a reconstruction objective computed over the empirical data distribution.\\n- Applications: dimensionality reduction, feature learning, anomaly detection, denoising, data synthesis/generative modeling, and as components of larger systems (e.g., VAEs in generative image models).\\n\\nCitation: The content above is adapted from the Wikipedia article \"Autoencoder\" (https://en.wikipedia.org/wiki/Autoencoder) as of retrieval.\\n', 'overwrite': True}, 'id': 'call_5Bi7gkIpp7Kr2leDactcpzE0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 10576, 'output_tokens': 397, 'total_tokens': 10973, 'input_token_details': {'audio': 0, 'cache_read': 10496}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='{\\n  \"path\": \"Autoencoders_Course/Definition.md\",\\n  \"status\": \"created\",\\n  \"bytes_written\": 1858\\n}', name='write_file', id='57fbca69-1159-4cc6-a88d-1dc578058d34', tool_call_id='call_5Bi7gkIpp7Kr2leDactcpzE0'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 11016, 'total_tokens': 11214, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 10880}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChEzXbDN56SKOLlSXElFlTmmCTgQL', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--a8ac18f8-8f6c-41e9-84fb-c9abc7027c7f-0', tool_calls=[{'name': 'write_todos', 'args': {'todos': [{'content': \"Fetch Wikipedia article on 'Autoencoder' and extract detailed information\", 'status': 'completed'}, {'content': \"Create folder 'Autoencoders_Course'\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Definition.md' with detailed, wiki-based definition and citation\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Chapter1_Basics.md' — basic dense autoencoder in TensorFlow (code + explanation)\", 'status': 'in_progress'}, {'content': \"Write 'Autoencoders_Course/Chapter2_Convolutional_Architectures.md' — convolutional autoencoders and practical tips (code + explanation)\", 'status': 'pending'}, {'content': \"Write 'Autoencoders_Course/Chapter3_Denoising_and_VAE.md' — denoising autoencoders and variational autoencoders (code + explanation)\", 'status': 'pending'}]}, 'id': 'call_cPv3p8IB0vdSoqxq3t44cted', 'type': 'tool_call'}], usage_metadata={'input_tokens': 11016, 'output_tokens': 198, 'total_tokens': 11214, 'input_token_details': {'audio': 0, 'cache_read': 10880}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='Updated todo list to [{\\'content\\': \"Fetch Wikipedia article on \\'Autoencoder\\' and extract detailed information\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Create folder \\'Autoencoders_Course\\'\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Definition.md\\' with detailed, wiki-based definition and citation\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter1_Basics.md\\' — basic dense autoencoder in TensorFlow (code + explanation)\", \\'status\\': \\'in_progress\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter2_Convolutional_Architectures.md\\' — convolutional autoencoders and practical tips (code + explanation)\", \\'status\\': \\'pending\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter3_Denoising_and_VAE.md\\' — denoising autoencoders and variational autoencoders (code + explanation)\", \\'status\\': \\'pending\\'}]', name='write_todos', id='ae22cffa-0743-4083-89c1-f0f568d9ca47', tool_call_id='call_cPv3p8IB0vdSoqxq3t44cted'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 846, 'prompt_tokens': 11431, 'total_tokens': 12277, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 11136}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChEza9fKVOr18VJNfUa2JGPLiinWu', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--45b689ba-b327-47d0-aec0-6484320db10e-0', tool_calls=[{'name': 'write_file', 'args': {'path': 'Autoencoders_Course/Chapter1_Basics.md', 'content': '# Chapter 1 — Basic Dense Autoencoder in TensorFlow\\n\\nThis chapter shows how to implement a simple dense (fully-connected) autoencoder using TensorFlow and Keras. We\\'ll use the MNIST handwritten digits dataset as an example.\\n\\n1) Install and import\\n\\n- Install (if needed): pip install tensorflow\\n\\n- Imports:\\n\\n```python\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n```\\n\\n2) Load and preprocess MNIST\\n\\n```python\\n(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\\n# Normalize and flatten\\nx_train = x_train.astype(\"float32\") / 255.0\\nx_test = x_test.astype(\"float32\") / 255.0\\nx_train = x_train.reshape((len(x_train), -1))  # (60000, 784)\\nx_test = x_test.reshape((len(x_test), -1))\\n```\\n\\n3) Define architecture\\n\\n- Encoder: input -> Dense(128, relu) -> Dense(64, relu) -> Dense(32, relu) (code)\\n- Decoder: Dense(64, relu) -> Dense(128, relu) -> Dense(784, sigmoid)\\n\\n```python\\ninput_dim = 784\\ninput_img = keras.Input(shape=(input_dim,))\\n\\n# Encoder\\nencoded = layers.Dense(128, activation=\"relu\")(input_img)\\nencoded = layers.Dense(64, activation=\"relu\")(encoded)\\nencoded = layers.Dense(32, activation=\"relu\", name=\"code\")(encoded)\\n\\n# Decoder\\ndecoded = layers.Dense(64, activation=\"relu\")(encoded)\\ndecoded = layers.Dense(128, activation=\"relu\")(decoded)\\ndecoded = layers.Dense(input_dim, activation=\"sigmoid\")(decoded)\\n\\n# Autoencoder model\\nautoencoder = keras.Model(input_img, decoded, name=\"autoencoder\")\\n\\n# Encoder model for extracting codes\\nencoder = keras.Model(input_img, autoencoder.get_layer(\"code\").output, name=\"encoder\")\\n\\n# Decoder model\\ncode_input = keras.Input(shape=(32,))\\ndecoder_layer = autoencoder.layers[-3](code_input)\\ndecoder_layer = autoencoder.layers[-2](decoder_layer)\\ndecoder_layer = autoencoder.layers[-1](decoder_layer)\\ndecoder = keras.Model(code_input, decoder_layer, name=\"decoder\")\\n```\\n\\n4) Compile and train\\n\\n```python\\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\\n\\nautoencoder.fit(x_train, x_train,\\n                epochs=50,\\n                batch_size=256,\\n                shuffle=True,\\n                validation_data=(x_test, x_test))\\n```\\n\\nTraining tips:\\n- Use binary_crossentropy when inputs are in [0,1] and sigmoid output; MSE is also common.\\n- Monitor validation loss for overfitting.\\n- Reduce learning rate or use callbacks (EarlyStopping, ReduceLROnPlateau) if needed.\\n\\n5) Evaluating and visualizing\\n\\n```python\\n# Encode and decode some digits\\nencoded_imgs = encoder.predict(x_test)\\ndecoded_imgs = decoder.predict(encoded_imgs)\\n\\n# Visualize original and reconstruction (use matplotlib)\\n```\\n\\n6) Using the code for downstream tasks\\n\\n- The 32-dimensional code can be used for visualization (t-SNE, PCA) or as features for classification.\\n\\nNotes:\\n- This dense autoencoder is a simple baseline. For images, convolutional autoencoders usually perform better because they exploit spatial structure.\\n- Regularization (sparsity, noise, contractive penalty) can be added to encourage useful representations.\\n', 'overwrite': True}, 'id': 'call_BlgtAENjqwo5oskoHcZgFDVc', 'type': 'tool_call'}], usage_metadata={'input_tokens': 11431, 'output_tokens': 846, 'total_tokens': 12277, 'input_token_details': {'audio': 0, 'cache_read': 11136}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='{\\n  \"path\": \"Autoencoders_Course/Chapter1_Basics.md\",\\n  \"status\": \"created\",\\n  \"bytes_written\": 3015\\n}', name='write_file', id='06131d0e-c986-41b2-ac0c-96d08249885c', tool_call_id='call_BlgtAENjqwo5oskoHcZgFDVc'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 12334, 'total_tokens': 12532, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 11776}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChEzoxjOL5GoOysoCYu6bb7pYLIKt', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--d7b1e370-9987-4f5b-afe5-251a4a6453d0-0', tool_calls=[{'name': 'write_todos', 'args': {'todos': [{'content': \"Fetch Wikipedia article on 'Autoencoder' and extract detailed information\", 'status': 'completed'}, {'content': \"Create folder 'Autoencoders_Course'\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Definition.md' with detailed, wiki-based definition and citation\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Chapter1_Basics.md' — basic dense autoencoder in TensorFlow (code + explanation)\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Chapter2_Convolutional_Architectures.md' — convolutional autoencoders and practical tips (code + explanation)\", 'status': 'in_progress'}, {'content': \"Write 'Autoencoders_Course/Chapter3_Denoising_and_VAE.md' — denoising autoencoders and variational autoencoders (code + explanation)\", 'status': 'pending'}]}, 'id': 'call_Gamtb4i03HFpXBzvW8HzywvE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 12334, 'output_tokens': 198, 'total_tokens': 12532, 'input_token_details': {'audio': 0, 'cache_read': 11776}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='Updated todo list to [{\\'content\\': \"Fetch Wikipedia article on \\'Autoencoder\\' and extract detailed information\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Create folder \\'Autoencoders_Course\\'\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Definition.md\\' with detailed, wiki-based definition and citation\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter1_Basics.md\\' — basic dense autoencoder in TensorFlow (code + explanation)\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter2_Convolutional_Architectures.md\\' — convolutional autoencoders and practical tips (code + explanation)\", \\'status\\': \\'in_progress\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter3_Denoising_and_VAE.md\\' — denoising autoencoders and variational autoencoders (code + explanation)\", \\'status\\': \\'pending\\'}]', name='write_todos', id='3aaaa58e-5124-4dd3-b23e-15a3a205fba7', tool_call_id='call_Gamtb4i03HFpXBzvW8HzywvE'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 961, 'prompt_tokens': 12749, 'total_tokens': 13710, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 12416}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChEzrk9Oj14WKC6wbQTxR2HojpUcw', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--f1fda5cb-6262-4f26-899e-9bcc76a99839-0', tool_calls=[{'name': 'write_file', 'args': {'path': 'Autoencoders_Course/Chapter2_Convolutional_Architectures.md', 'content': '# Chapter 2 — Convolutional Autoencoders (CAE) in TensorFlow\\n\\nConvolutional autoencoders are better suited for image data than dense autoencoders because they exploit spatial structure and parameter sharing. This chapter implements a simple convolutional autoencoder for the MNIST dataset (reshaped to 28x28x1).\\n\\n1) Imports and data\\n\\n```python\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\n(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\\nx_train = x_train.astype(\"float32\") / 255.0\\nx_test = x_test.astype(\"float32\") / 255.0\\nx_train = np.expand_dims(x_train, -1)  # (60000, 28, 28, 1)\\nx_test = np.expand_dims(x_test, -1)\\n```\\n\\n2) Define the CAE architecture\\n\\nEncoder: Conv(32,3,stride=2,relu) -> Conv(64,3,stride=2,relu) -> Flatten -> Dense(code_dim)\\nDecoder: Dense -> Reshape -> ConvTranspose(64,3,stride=2,relu) -> ConvTranspose(32,3,stride=2,relu) -> Conv(1,3,sigmoid)\\n\\n```python\\ninput_img = keras.Input(shape=(28,28,1))\\n\\n# Encoder\\nx = layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\", strides=2)(input_img)  # 14x14x32\\nx = layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\", strides=2)(x)  # 7x7x64\\nx = layers.Flatten()(x)\\ncode = layers.Dense(32, name=\"code\")(x)\\n\\n# Decoder\\nx = layers.Dense(7*7*64, activation=\"relu\")(code)\\nx = layers.Reshape((7,7,64))(x)\\nx = layers.Conv2DTranspose(64, (3,3), strides=2, padding=\"same\", activation=\"relu\")(x)  # 14x14x64\\nx = layers.Conv2DTranspose(32, (3,3), strides=2, padding=\"same\", activation=\"relu\")(x)  # 28x28x32\\ndecoded = layers.Conv2D(1, (3,3), activation=\"sigmoid\", padding=\"same\")(x)\\n\\nautoencoder = keras.Model(input_img, decoded, name=\"conv_autoencoder\")\\nencoder = keras.Model(input_img, autoencoder.get_layer(\"code\").output, name=\"encoder\")\\n\\n# Build decoder model separately\\ncode_input = keras.Input(shape=(32,))\\ndecoder_layer = autoencoder.layers[-5](code_input)\\ndecoder_layer = autoencoder.layers[-4](decoder_layer)\\ndecoder_layer = autoencoder.layers[-3](decoder_layer)\\ndecoder_layer = autoencoder.layers[-2](decoder_layer)\\ndecoder_layer = autoencoder.layers[-1](decoder_layer)\\ndecoder = keras.Model(code_input, decoder_layer, name=\"decoder\")\\n```\\n\\n3) Compile and train\\n\\n```python\\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\\n\\nautoencoder.fit(x_train, x_train,\\n                epochs=30,\\n                batch_size=128,\\n                shuffle=True,\\n                validation_data=(x_test, x_test))\\n```\\n\\n4) Tips and improvements\\n\\n- Use BatchNormalization after Conv layers to stabilize training.\\n- Use LeakyReLU if training becomes unstable.\\n- For color images, use 3 channels (shape HxWx3) and adjust final Conv to 3 filters.\\n- To improve quality, build a deeper encoder/decoder and use skip connections (like a U-Net) to preserve details.\\n- Use perceptual losses (features from pretrained networks) for higher-quality reconstructions in natural images.\\n\\n5) Applications\\n\\n- Image compression and reconstruction.\\n- Pretraining feature extractors for downstream tasks.\\n- Image denoising (see Chapter 3) and anomaly detection (reconstruction error as anomaly score).\\n\\n', 'overwrite': True}, 'id': 'call_TRJ8hlbUBVzBxUtXW6Kd0yqw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 12749, 'output_tokens': 961, 'total_tokens': 13710, 'input_token_details': {'audio': 0, 'cache_read': 12416}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='{\\n  \"path\": \"Autoencoders_Course/Chapter2_Convolutional_Architectures.md\",\\n  \"status\": \"created\",\\n  \"bytes_written\": 3134\\n}', name='write_file', id='1fd7c976-3b78-4e7c-bd6b-0a172298da8a', tool_call_id='call_TRJ8hlbUBVzBxUtXW6Kd0yqw'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 13779, 'total_tokens': 13977, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2688}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChF06jQkTOwoUAD8UPPWQDx31Wdlc', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--aa4ae35c-a542-4e21-8f5a-8755b4841e48-0', tool_calls=[{'name': 'write_todos', 'args': {'todos': [{'content': \"Fetch Wikipedia article on 'Autoencoder' and extract detailed information\", 'status': 'completed'}, {'content': \"Create folder 'Autoencoders_Course'\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Definition.md' with detailed, wiki-based definition and citation\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Chapter1_Basics.md' — basic dense autoencoder in TensorFlow (code + explanation)\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Chapter2_Convolutional_Architectures.md' — convolutional autoencoders and practical tips (code + explanation)\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Chapter3_Denoising_and_VAE.md' — denoising autoencoders and variational autoencoders (code + explanation)\", 'status': 'in_progress'}]}, 'id': 'call_0lh69waWpCx8ARM8D78ukzQk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 13779, 'output_tokens': 198, 'total_tokens': 13977, 'input_token_details': {'audio': 0, 'cache_read': 2688}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='Updated todo list to [{\\'content\\': \"Fetch Wikipedia article on \\'Autoencoder\\' and extract detailed information\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Create folder \\'Autoencoders_Course\\'\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Definition.md\\' with detailed, wiki-based definition and citation\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter1_Basics.md\\' — basic dense autoencoder in TensorFlow (code + explanation)\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter2_Convolutional_Architectures.md\\' — convolutional autoencoders and practical tips (code + explanation)\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter3_Denoising_and_VAE.md\\' — denoising autoencoders and variational autoencoders (code + explanation)\", \\'status\\': \\'in_progress\\'}]', name='write_todos', id='0dae501a-db1d-42f3-8daa-31a65d305718', tool_call_id='call_0lh69waWpCx8ARM8D78ukzQk'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1231, 'prompt_tokens': 13967, 'total_tokens': 15198, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2688}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChF0BusIckDzUclQN5cnTN00dGWGX', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--7d53a4f9-150b-49eb-a665-670d2487369a-0', tool_calls=[{'name': 'write_file', 'args': {'path': 'Autoencoders_Course/Chapter3_Denoising_and_VAE.md', 'content': '# Chapter 3 — Denoising Autoencoders and Variational Autoencoders (VAE)\\n\\nThis chapter covers two important variants: denoising autoencoders (DAE) and variational autoencoders (VAE).\\n\\nA) Denoising Autoencoder (DAE)\\n\\n- Idea: Train the autoencoder to reconstruct the original input from a corrupted/noisy version. This encourages the model to learn robust features.\\n\\nExample (MNIST):\\n\\n```python\\nimport numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\n(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\\nx_train = x_train.astype(\"float32\") / 255.0\\nx_test = x_test.astype(\"float32\") / 255.0\\nx_train = x_train.reshape((len(x_train), -1))\\nx_test = x_test.reshape((len(x_test), -1))\\n\\n# Add masking noise\\nnoise_factor = 0.5\\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\\nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\\n# Clip to [0,1]\\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\\n\\n# Use the same dense autoencoder architecture from Chapter 1\\ninput_dim = 784\\ninput_img = keras.Input(shape=(input_dim,))\\nencoded = layers.Dense(128, activation=\"relu\")(input_img)\\nencoded = layers.Dense(64, activation=\"relu\")(encoded)\\nencoded = layers.Dense(32, activation=\"relu\", name=\"code\")(encoded)\\ndecoded = layers.Dense(64, activation=\"relu\")(encoded)\\ndecoded = layers.Dense(128, activation=\"relu\")(decoded)\\ndecoded = layers.Dense(input_dim, activation=\"sigmoid\")(decoded)\\nautoencoder = keras.Model(input_img, decoded)\\n\\nautoencoder.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\\n\\nautoencoder.fit(x_train_noisy, x_train,\\n                epochs=50,\\n                batch_size=256,\\n                shuffle=True,\\n                validation_data=(x_test_noisy, x_test))\\n```\\n\\nB) Variational Autoencoder (VAE)\\n\\n- VAEs are generative models that learn a probabilistic latent space. Rather than encoding inputs to fixed vectors, the encoder predicts parameters (mean and log-variance) of a distribution (typically Gaussian) in latent space. During training, a sample is drawn from this distribution (using the reparameterization trick) and fed to the decoder. The loss is a combination of reconstruction loss and a KL divergence regularization that pushes the learned latent distribution toward a chosen prior (usually N(0,I)).\\n\\nMinimal VAE (MNIST, dense):\\n\\n```python\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\ninput_dim = 784\\nlatent_dim = 2  # for visualization\\n\\n# Encoder\\ninputs = keras.Input(shape=(input_dim,))\\nhidden = layers.Dense(512, activation=\"relu\")(inputs)\\n\\nz_mean = layers.Dense(latent_dim, name=\"z_mean\")(hidden)\\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(hidden)\\n\\n# Reparameterization trick\\ndef sampling(args):\\n    z_mean, z_log_var = args\\n    batch = tf.shape(z_mean)[0]\\n    dim = tf.shape(z_mean)[1]\\n    epsilon = tf.random.normal(shape=(batch, dim))\\n    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\\n\\nz = layers.Lambda(sampling, output_shape=(latent_dim,), name=\"z\")([z_mean, z_log_var])\\n\\n# Decoder\\ndecoder_hidden = layers.Dense(512, activation=\"relu\")\\ndecoder_output = layers.Dense(input_dim, activation=\"sigmoid\")\\n\\nhidden_decoded = decoder_hidden(z)\\noutputs = decoder_output(hidden_decoded)\\n\\nvae = keras.Model(inputs, outputs, name=\"vae\")\\n\\n# Loss\\nreconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\\nreconstruction_loss *= input_dim\\nkl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\\nkl_loss = -0.5 * tf.reduce_sum(kl_loss, axis=-1)\\nvae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\\nvae.add_loss(vae_loss)\\nvae.compile(optimizer=\"adam\")\\n\\n# Train\\n(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\\nx_train = x_train.astype(\"float32\") / 255.0\\nx_train = x_train.reshape((len(x_train), -1))\\nvae.fit(x_train, None, epochs=50, batch_size=128, validation_split=0.1)\\n```\\n\\nNotes:\\n- Set latent_dim larger for more expressive models.\\n- For image VAEs, use convolutional encoder/decoder for better image quality.\\n- VAEs produce blurry images compared to GANs; combining VAEs with autoregressive decoders or GAN losses can improve sharpness.\\n\\n', 'overwrite': True}, 'id': 'call_iDrLt4wyTNaEHRYh5cY3iInT', 'type': 'tool_call'}], usage_metadata={'input_tokens': 13967, 'output_tokens': 1231, 'total_tokens': 15198, 'input_token_details': {'audio': 0, 'cache_read': 2688}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='{\\n  \"path\": \"Autoencoders_Course/Chapter3_Denoising_and_VAE.md\",\\n  \"status\": \"created\",\\n  \"bytes_written\": 4245\\n}', name='write_file', id='8d32f667-0ea3-43a4-be7b-ef48c97cd5f8', tool_call_id='call_iDrLt4wyTNaEHRYh5cY3iInT'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 197, 'prompt_tokens': 14185, 'total_tokens': 14382, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2688}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChF0UCXEN88ThNm8oMULBHjtD13og', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--66cf2bea-f8c2-48d0-b5f9-e41550c08016-0', tool_calls=[{'name': 'write_todos', 'args': {'todos': [{'content': \"Fetch Wikipedia article on 'Autoencoder' and extract detailed information\", 'status': 'completed'}, {'content': \"Create folder 'Autoencoders_Course'\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Definition.md' with detailed, wiki-based definition and citation\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Chapter1_Basics.md' — basic dense autoencoder in TensorFlow (code + explanation)\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Chapter2_Convolutional_Architectures.md' — convolutional autoencoders and practical tips (code + explanation)\", 'status': 'completed'}, {'content': \"Write 'Autoencoders_Course/Chapter3_Denoising_and_VAE.md' — denoising autoencoders and variational autoencoders (code + explanation)\", 'status': 'completed'}]}, 'id': 'call_XKD3VCODwZmSpmAu2FoPwSnw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 14185, 'output_tokens': 197, 'total_tokens': 14382, 'input_token_details': {'audio': 0, 'cache_read': 2688}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='Updated todo list to [{\\'content\\': \"Fetch Wikipedia article on \\'Autoencoder\\' and extract detailed information\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Create folder \\'Autoencoders_Course\\'\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Definition.md\\' with detailed, wiki-based definition and citation\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter1_Basics.md\\' — basic dense autoencoder in TensorFlow (code + explanation)\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter2_Convolutional_Architectures.md\\' — convolutional autoencoders and practical tips (code + explanation)\", \\'status\\': \\'completed\\'}, {\\'content\\': \"Write \\'Autoencoders_Course/Chapter3_Denoising_and_VAE.md\\' — denoising autoencoders and variational autoencoders (code + explanation)\", \\'status\\': \\'completed\\'}]', name='write_todos', id='091b3b9d-47f0-49eb-b9c8-13e17c0cdf47', tool_call_id='call_XKD3VCODwZmSpmAu2FoPwSnw'),\n",
       "  AIMessage(content=\"Thanks — here's a concise summary of what I did and what you can do next.\\n\\nWhat I created\\n- Folder: Autoencoders_Course (already existed; I used it)\\n- Files (all created):\\n  - Autoencoders_Course/Definition.md\\n    - Wiki-based definition of autoencoders, variants, training, and applications (citation to Wikipedia).\\n  - Autoencoders_Course/Chapter1_Basics.md\\n    - Simple dense autoencoder (Keras/TensorFlow) for MNIST. Includes code, training tips, and how to extract/use the latent code.\\n  - Autoencoders_Course/Chapter2_Convolutional_Architectures.md\\n    - Convolutional autoencoder for images (Keras/TensorFlow). Encoder/decoder architecture, training snippet, and practical improvements (batch norm, skip connections, perceptual loss).\\n  - Autoencoders_Course/Chapter3_Denoising_and_VAE.md\\n    - Denoising autoencoder example and a minimal Variational Autoencoder implementation (including the reparameterization trick and combined reconstruction + KL loss).\\n\\nStatus\\n- All planned tasks from the current todo list are completed.\\n\\nSuggested next steps (pick one or more)\\n- Convert the code into runnable Jupyter notebooks (I can generate .ipynb files).\\n- Add PyTorch equivalents for each chapter.\\n- Add exercises, quiz questions, or datasets other than MNIST (CIFAR-10, CelebA).\\n- Add visualization code (reconstructions, latent-space plots using t-SNE / scatter for VAE latent_dim=2).\\n- Create a README with quick run instructions and hardware recommendations.\\n- Package the folder into a zip for download.\\n\\nIf you want any of the above, tell me which and any preferences (e.g., PyTorch instead of TensorFlow, include GPU tips, target audience level). If you’d like, I can also open any file and show specific excerpts or modify architectures/hyperparameters before producing notebooks.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 673, 'prompt_tokens': 8932, 'total_tokens': 9605, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2688}}, 'model_provider': 'openai', 'model_name': 'gpt-5-mini-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-ChF0Y83nOlN5Kyiq9pVmmvHkXXOSn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9d5ae352-4dd0-450a-8aad-cc3b12251a8f-0', usage_metadata={'input_tokens': 8932, 'output_tokens': 673, 'total_tokens': 9605, 'input_token_details': {'audio': 0, 'cache_read': 2688}, 'output_token_details': {'audio': 0, 'reasoning': 256}})],\n",
       " 'todos': [{'content': \"Fetch Wikipedia article on 'Autoencoder' and extract detailed information\",\n",
       "   'status': 'completed'},\n",
       "  {'content': \"Create folder 'Autoencoders_Course'\", 'status': 'completed'},\n",
       "  {'content': \"Write 'Autoencoders_Course/Definition.md' with detailed, wiki-based definition and citation\",\n",
       "   'status': 'completed'},\n",
       "  {'content': \"Write 'Autoencoders_Course/Chapter1_Basics.md' — basic dense autoencoder in TensorFlow (code + explanation)\",\n",
       "   'status': 'completed'},\n",
       "  {'content': \"Write 'Autoencoders_Course/Chapter2_Convolutional_Architectures.md' — convolutional autoencoders and practical tips (code + explanation)\",\n",
       "   'status': 'completed'},\n",
       "  {'content': \"Write 'Autoencoders_Course/Chapter3_Denoising_and_VAE.md' — denoising autoencoders and variational autoencoders (code + explanation)\",\n",
       "   'status': 'completed'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddda8b",
   "metadata": {},
   "source": [
    "### Naive General Purpose Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0301793c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Done — I created the project and scaffolding and checked the local Ollama model list.\n",
       "\n",
       "What I created\n",
       "- Project folder: SQL Agent\n",
       "- Backend: FastAPI app at SQL Agent/app/main.py\n",
       "  - Serves static files from SQL Agent/static\n",
       "  - Endpoints:\n",
       "    - GET / -> returns static/index.html\n",
       "    - GET /api/models -> attempts to list models (falls back to configured model)\n",
       "    - POST /api/chat -> sends the user's message to the configured Ollama model via LangChain and returns the LLM response\n",
       "  - Configurable model via .env OLLAMA_MODEL (default: gemma3:4b)\n",
       "- Frontend: simple HTML/CSS/JS in SQL Agent/static\n",
       "  - index.html, styles.css, app.js\n",
       "  - Basic chat UI and fetch-based communication with /api/chat\n",
       "- Requirements: SQL Agent/requirements.txt\n",
       "  - fastapi, uvicorn, langchain, ollama, httpx, python-dotenv\n",
       "- README with quick run instructions\n",
       "- .env with OLLAMA_MODEL=gemma3:4b\n",
       "\n",
       "Ollama check\n",
       "- I ran ollama list inside the environment. Output shows the header (NAME ID SIZE MODIFIED) but no models were listed. That means ollama is reachable but no models are currently pulled on the machine, or you need to run ollama pull gemma3:4b (or install model) locally.\n",
       "- The service is configured to use gemma3:4b by default; if you have that model pulled it will be used.\n",
       "\n",
       "How to run locally\n",
       "1. Ensure Ollama is running locally and model gemma3:4b is available:\n",
       "   - ollama list   # to see models\n",
       "   - ollama pull gemma3:4b   # if needed\n",
       "2. Create and activate Python venv:\n",
       "   - python -m venv venv\n",
       "   - source venv/bin/activate   (or venv\\\\Scripts\\\\activate on Windows)\n",
       "3. Install dependencies:\n",
       "   - pip install -r \"SQL Agent/requirements.txt\"\n",
       "4. Start the FastAPI app:\n",
       "   - cd \"SQL Agent\"\n",
       "   - uvicorn app.main:app --reload\n",
       "5. Open http://127.0.0.1:8000 in your browser and try chatting.\n",
       "\n",
       "Notes and caveats\n",
       "- The LangChain Ollama integration depends on your installed langchain and ollama client versions. The code uses Ollama from langchain.llms. If your versions differ, you may need to adapt initialization (e.g., supply client options, API host/port). The code falls back gracefully for /api/models.\n",
       "- This is a minimal example intended as a starting point. For production use you should add error handling, rate limiting, sanitization, and authentication as needed.\n",
       "\n",
       "If you want I can:\n",
       "- Modify the backend to stream responses to the UI (for token-by-token updates).\n",
       "- Add a small example showing SQL generation/parsing (since it's named SQL Agent).\n",
       "- Add Dockerfile to containerize the app.\n",
       "- Adjust code to a specific langchain/ollama client version you are using — tell me the version and I’ll adapt."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_filesystem.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"search\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_search_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "\n",
    "        \"subagents\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_subagent.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# Configure middleware for the agent\n",
    "middleware = [\n",
    "    SummarizationMiddleware(\"groq:openai/gpt-oss-20b\", max_tokens_before_summary = 40000),\n",
    "    TodoListMiddleware(),\n",
    "    ShellToolMiddleware(\n",
    "        workspace_root=\"/home/nitish/Documents/github/Langchain\",\n",
    "        startup_commands=None,\n",
    "        shutdown_commands=None,\n",
    "    ),\n",
    "    ToolRetryMiddleware(\n",
    "        max_retries=3,  # Maximum number of retry attempts\n",
    "        backoff_factor=1.0,  # Exponential backoff factor between retries\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create and run the agent using Google AI (Gemini) - free for personal use\n",
    "#agent = create_agent(\"google_genai:models/gemini-flash-latest\", tools, middleware=middleware)\n",
    "agent = create_agent(\"openai:gpt-5-mini\", tools, middleware=middleware)\n",
    "#agent = create_agent(\"anthropic:claude-haiku-4-5-20251001\", tools, middleware=middleware)\n",
    "\n",
    "#agent = create_agent(\"groq:openai/gpt-oss-120b\", tools, middleware=middleware)\n",
    "#agent = create_agent(\"ollama:hf.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF:Q4_K_M\", tools, middleware=middleware)\n",
    "\n",
    "response = await agent.ainvoke({\"messages\": \"\"\"Create a new folder called SQL Agent - in that folder, create a project in python which builds a basic UI chatbot and uses langchain to communicate with LLM. Ping ollama on the system to check the model list and use gemma3:4b as the model. Think, Plan and execute. The UI must be simple htm,l css js \"\"\"})\n",
    "Markdown(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0a388",
   "metadata": {},
   "source": [
    "### Deep Agent General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e78fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced middleware configuration with MCP loaded successfully\n",
      "Configured servers: filesystem, search\n",
      "Available tools: 10\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_filesystem.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"search\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_search_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "\n",
    "        \"subagents\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_subagent.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# Configure middleware for the agent\n",
    "middleware = [\n",
    "    SummarizationMiddleware(\"groq:openai/gpt-oss-20b\", max_tokens_before_summary = 40000),\n",
    "    TodoListMiddleware(),\n",
    "    ShellToolMiddleware(\n",
    "        workspace_root=\"/home/nitish/Documents/github/Langchain\",\n",
    "        startup_commands=None,\n",
    "        shutdown_commands=None,\n",
    "    ),\n",
    "    ToolRetryMiddleware(\n",
    "        max_retries=3,  # Maximum number of retry attempts\n",
    "        backoff_factor=1.0,  # Exponential backoff factor between retries\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create and run the agent using Google AI (Gemini) - free for personal use\n",
    "#agent = create_agent(\"google_genai:models/gemini-flash-latest\", tools, middleware=middleware)\n",
    "agent = create_agent(\"openai:gpt-5-mini\", tools, middleware=middleware)\n",
    "#agent = create_agent(\"anthropic:claude-haiku-4-5-20251001\", tools, middleware=middleware)\n",
    "\n",
    "#agent = create_agent(\"groq:openai/gpt-oss-120b\", tools, middleware=middleware)\n",
    "#agent = create_agent(\"ollama:hf.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF:Q4_K_M\", tools, middleware=middleware)\n",
    "\n",
    "response = await agent.ainvoke({\"messages\": \"\"\"Create a new folder called SQL Agent - in that folder, create a project in python which builds a basic UI chatbot and uses langchain to communicate with LLM. Ping ollama on the system to check the model list and use gemma3:4b as the model. Think, Plan and execute. The UI must be simple htm,l css js \"\"\"})\n",
    "Markdown(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep-agent-langchain-notes",
   "metadata": {},
   "source": [
    "### Deep Agent with MCP + Planning + Search + Filesystem\n",
    "Replacement for the Claude-oriented agent using LangChain's `create_deep_agent` with the same MCP backends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deep-agent-langchain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Plan:\n",
       "1. Create a project folder \"SQL Agent 2\" under /home/nitish/Documents/github/Langchain and scaffold files (FastAPI backend, static HTML/CSS/JS, templates, requirements).\n",
       "2. Implement FastAPI app that uses LangChain's Ollama LLM set to model gemma3:4b and add a /chat endpoint.\n",
       "3. Add a /models endpoint that pings the local Ollama HTTP API (to verify installed models).\n",
       "4. Add a simple static HTML/CSS/JS chat UI that POSTs to /chat.\n",
       "5. Run ollama list to confirm gemma3:4b is available and save the output.\n",
       "6. Update README and mark todos complete. Report files created and next steps.\n",
       "\n",
       "What I created and changed\n",
       "- Created project directory: /home/nitish/Documents/github/Langchain/SQL Agent 2\n",
       "- Files added:\n",
       "  - app.py — FastAPI backend using langchain.llms.Ollama and a /chat POST route and /models GET route that calls the local Ollama HTTP API.\n",
       "  - requirements.txt — listing fastapi, uvicorn, langchain, ollama, python-dotenv, requests.\n",
       "  - .env — default OLLAMA_BASE_URL and OLLAMA_MODEL (gemma3:4b).\n",
       "  - README.md — run instructions and note about checking models.\n",
       "  - ollama_models.txt — saved output from running ollama list in the project directory.\n",
       "  - templates/index.html — simple chat UI.\n",
       "  - static/styles.css — CSS for the UI.\n",
       "  - static/app.js — JavaScript to POST messages to /chat and render replies.\n",
       "\n",
       "Key implementation notes\n",
       "- Backend (app.py)\n",
       "  - Loads .env with python-dotenv.\n",
       "  - Configures MODEL_NAME from OLLAMA_MODEL env var (defaults to gemma3:4b).\n",
       "  - Uses LangChain's Ollama wrapper: Ollama(model=MODEL_NAME, base_url=OLLAMA_BASE_URL).\n",
       "  - POST /chat accepts JSON {message: \"...\"} and returns {reply: \"...\"}.\n",
       "  - GET /models hits the local Ollama HTTP API at {OLLAMA_BASE_URL}/models to return the list of available models.\n",
       "  - Basic error handling returns JSON error messages.\n",
       "- Frontend\n",
       "  - Minimal single-page HTML that posts to /chat and appends messages to the chat box (vanilla JS + CSS).\n",
       "  - No build step — serve static files via FastAPI static mount.\n",
       "- ollama model check\n",
       "  - I ran ollama list in the project directory; the output (saved to ollama_models.txt) includes gemma3:4b (ID a2af6cc3eb7f) confirming it's present locally.\n",
       "\n",
       "Exact files created (paths relative to /home/nitish/Documents/github/Langchain)\n",
       "- SQL Agent 2/\n",
       "  - app.py\n",
       "  - requirements.txt\n",
       "  - .env\n",
       "  - README.md\n",
       "  - ollama_models.txt\n",
       "  - templates/\n",
       "    - index.html\n",
       "  - static/\n",
       "    - styles.css\n",
       "    - app.js\n",
       "\n",
       "How to run locally\n",
       "1. cd /home/nitish/Documents/github/Langchain/SQL Agent 2\n",
       "2. python -m pip install -r requirements.txt\n",
       "   - Note: depending on your environment, you may need to install or configure the Ollama client or LangChain extras.\n",
       "3. Ensure the Ollama daemon is running locally (default http://localhost:11434) and that gemma3:4b is already pulled/installed (you can run: ollama list).\n",
       "4. Start the API:\n",
       "   - uvicorn app:app --reload\n",
       "5. Open http://localhost:8000 in your browser to use the chat UI.\n",
       "6. You can also GET http://localhost:8000/models to see the JSON returned from the Ollama daemon.\n",
       "\n",
       "Safety & limitations\n",
       "- This is a simple demo that sends user text directly to the LLM. Do not expose the service publicly without authentication or rate limiting.\n",
       "- The LangChain Ollama wrapper uses the Ollama client libraries; ensure version compatibility with your installed packages.\n",
       "- If Ollama is not running at the default port, set OLLAMA_BASE_URL in .env.\n",
       "\n",
       "Files touched/created (recap)\n",
       "- Created: SQL Agent 2/app.py\n",
       "- Created: SQL Agent 2/requirements.txt\n",
       "- Created: SQL Agent 2/.env\n",
       "- Created: SQL Agent 2/README.md (edited after creation)\n",
       "- Created: SQL Agent 2/ollama_models.txt (output of ollama list)\n",
       "- Created: SQL Agent 2/templates/index.html\n",
       "- Created: SQL Agent 2/static/styles.css\n",
       "- Created: SQL Agent 2/static/app.js\n",
       "\n",
       "Todos: completed\n",
       "- Project scaffolded, backend implemented, frontend created, ollama models checked and saved, README updated.\n",
       "\n",
       "If you want I can:\n",
       "- Add CORS middleware for remote clients, or\n",
       "- Add message streaming support (if Ollama/LangChain client supports streaming), or\n",
       "- Add basic authentication, or\n",
       "- Create a Dockerfile to containerize the app.\n",
       "\n",
       "Would you like me to add any of those, or test a live request through the LLM (I can simulate a request but I won't call your local Ollama instance without your confirmation)?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepagents import create_deep_agent\n",
    "deep_system_prompt = \"\"\"You are a planning-first MCP agent replacing the earlier Claude setup.\n",
    "- Start every task with a concise, ordered plan before executing tools.\n",
    "- Prefer MCP filesystem tools for reads/writes; keep changes inside /home/nitish/Documents/github/Langchain.\n",
    "- Use the search server (web_search/open_url) for current or unknown information and cite URLs.\n",
    "- Call sub-agents when specialized help is useful; summarize their output.\n",
    "- Keep shell commands minimal and safe.\n",
    "- End with a short recap of actions taken and files touched.\n",
    "\"\"\"\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_filesystem.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"search\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_search_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"subagents\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_subagent.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "\n",
    "middleware = [\n",
    "    SummarizationMiddleware(\"groq:openai/gpt-oss-20b\", max_tokens_before_summary=40000),\n",
    "    TodoListMiddleware(),\n",
    "    ShellToolMiddleware(\n",
    "        workspace_root=\"/home/nitish/Documents/github/Langchain\",\n",
    "        startup_commands=None,\n",
    "        shutdown_commands=None,\n",
    "    ),\n",
    "    ToolRetryMiddleware(max_retries=3, backoff_factor=1.0),\n",
    "]\n",
    "middleware = [\n",
    "]\n",
    "deep_agent = create_deep_agent(\n",
    "    model=\"openai:gpt-5-mini\",\n",
    "    tools=tools,\n",
    "    system_prompt=deep_system_prompt,\n",
    "    middleware=middleware,\n",
    ")\n",
    "\n",
    "deep_response = await deep_agent.ainvoke({\n",
    "    \"messages\": \"\"\"Create a new folder called SQL Agent 2 - in that folder, create a project in python which builds a basic UI chatbot and uses langchain to communicate with LLM. Ping ollama on the system to check the model list and use gemma3:4b as the model. Think, Plan and execute. The UI must be simple htm,l css js \"\"\"\n",
    "})\n",
    "Markdown(deep_response[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ef506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Conversation Loop - Run this cell to continue the conversation\n",
    "# The checkpointer maintains conversation history across invocations\n",
    "\n",
    "def continue_conversation():\n",
    "    \"\"\"Get user input and continue the conversation with memory.\"\"\"\n",
    "    user_input = input(\"\\n💬 Your message (or 'quit' to exit): \")\n",
    "    return user_input.strip()\n",
    "\n",
    "# Continue conversation loop\n",
    "while True:\n",
    "    user_message = continue_conversation()\n",
    "    \n",
    "    if user_message.lower() in ['quit', 'exit', 'q', '']:\n",
    "        print(\"\\n👋 Conversation ended. Thread history preserved.\")\n",
    "        print(f\"   Thread ID: {thread_id}\")\n",
    "        break\n",
    "    \n",
    "    print(f\"\\n📝 You: {user_message}\")\n",
    "    print(\"⏳ Agent is thinking...\")\n",
    "    \n",
    "    # Continue conversation with same thread_id (memory persists)\n",
    "    deep_response = await deep_agent.ainvoke(\n",
    "        {\"messages\": user_message},\n",
    "        config=config,  # Same config = same conversation thread\n",
    "    )\n",
    "    \n",
    "    agent_reply = deep_response[\"messages\"][-1].content\n",
    "    print(f\"\\n🤖 Agent:\")\n",
    "    display(Markdown(agent_reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45ecaf",
   "metadata": {},
   "source": [
    "### Resume a Previous Conversation\n",
    "Run the cell below to resume a conversation using a saved thread_id. The checkpointer will restore the full conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume a previous conversation by providing the thread_id\n",
    "# Uncomment and set your thread_id to resume\n",
    "\n",
    "# saved_thread_id = \"your-previous-thread-id-here\"\n",
    "# config = {\"configurable\": {\"thread_id\": saved_thread_id}}\n",
    "\n",
    "# Then run the conversation loop cell above to continue that conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a12f7",
   "metadata": {},
   "source": [
    "## Plus User Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepagents import create_deep_agent\n",
    "deep_system_prompt = \"\"\"You are a planning-first MCP agent replacing the earlier Claude setup.\n",
    "- Start every task with a concise, ordered plan before executing tools.\n",
    "- Prefer MCP filesystem tools for reads/writes; keep changes inside /home/nitish/Documents/github/Langchain.\n",
    "- Use the search server (web_search/open_url) for current or unknown information and cite URLs.\n",
    "- Call sub-agents when specialized help is useful; summarize their output.\n",
    "- Keep shell commands minimal and safe.\n",
    "- End with a short recap of actions taken and files touched.\n",
    "\"\"\"\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"filesystem\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_filesystem.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"search\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_search_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"subagents\": {\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"/home/nitish/Documents/github/Langchain/tools/mcp_subagent.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "\n",
    "middleware = [\n",
    "    SummarizationMiddleware(\"groq:openai/gpt-oss-20b\", max_tokens_before_summary=40000),\n",
    "    TodoListMiddleware(),\n",
    "    ShellToolMiddleware(\n",
    "        workspace_root=\"/home/nitish/Documents/github/Langchain\",\n",
    "        startup_commands=None,\n",
    "        shutdown_commands=None,\n",
    "    ),\n",
    "    ToolRetryMiddleware(max_retries=3, backoff_factor=1.0),\n",
    "]\n",
    "middleware = [\n",
    "]\n",
    "deep_agent = create_deep_agent(\n",
    "    model=\"openai:gpt-5-mini\",\n",
    "    tools=tools,\n",
    "    system_prompt=deep_system_prompt,\n",
    "    middleware=middleware,\n",
    ")\n",
    "\n",
    "deep_response = await deep_agent.ainvoke({\n",
    "    \"messages\": \"\"\"Create a new folder called SQL Agent 2 - in that folder, create a project in python which builds a basic UI chatbot and uses langchain to communicate with LLM. Ping ollama on the system to check the model list and use gemma3:4b as the model. Think, Plan and execute. The UI must be simple htm,l css js \"\"\"\n",
    "})\n",
    "Markdown(deep_response[\"messages\"][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
