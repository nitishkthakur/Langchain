{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReAct Agent with LangChain\n",
    "\n",
    "This notebook demonstrates how to create and use a ReAct (Reasoning and Acting) agent with LangChain.\n",
    "\n",
    "ReAct agents combine reasoning and acting by allowing the LLM to:\n",
    "1. **Reason** about what action to take\n",
    "2. **Act** by calling tools\n",
    "3. **Observe** the results\n",
    "4. Repeat until the task is complete\n",
    "\n",
    "## What You'll Learn:\n",
    "- Setting up a ReAct agent\n",
    "- Integrating tools (Tavily Search)\n",
    "- Running agent workflows\n",
    "- Understanding agent decision-making process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import (\n",
    "    SummarizationMiddleware,\n",
    "    ToolRetryMiddleware,\n",
    "    ModelFallbackMiddleware,\n",
    "    LLMToolSelectorMiddleware,\n",
    ")\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Optional backend helpers (installed when deploying)\n",
    "try:\n",
    "    from fastapi import FastAPI\n",
    "    from langserve import add_routes\n",
    "except ImportError:\n",
    "    FastAPI = None\n",
    "    add_routes = None\n",
    "\n",
    "# Import our custom Tavily search tool\n",
    "from tools.tavily_search import get_tavily_search_tool\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify LangChain 1.0.5+ compatibility\n",
    "import langchain\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "if tuple(map(int, langchain.__version__.split('.')[:2])) >= (1, 0):\n",
    "    print(\"✓ LangChain 1.0+ detected — all middleware and agent patterns are supported.\")\n",
    "else:\n",
    "    print(\"⚠️ LangChain < 1.0 detected. Please upgrade: pip install -U langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up API Keys\n",
    "\n",
    "You'll need:\n",
    "- OpenAI API key (for the LLM)\n",
    "- Tavily API key (for web search)\n",
    "\n",
    "Set these as environment variables or in a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API keys are configured!\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and set your API keys if not using .env file\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"your-tavily-api-key\"\n",
    "\n",
    "# Verify keys are set\n",
    "if \"OPENAI_API_KEY\" in os.environ and \"TAVILY_API_KEY\" in os.environ:\n",
    "    print(\"✓ API keys are configured!\")\n",
    "else:\n",
    "    print(\"⚠ Warning: Please set your API keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the LLM\n",
    "\n",
    "We'll use OpenAI's GPT-4 model as our reasoning engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LLM initialized: gpt-5-nano\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM\n",
    "# Note: Using model string (auto-inferred) or explicit ChatOpenAI instance\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Use gpt-4o-mini instead of gpt-5-nano for better cost/performance\n",
    "    temperature=0.3,  # Lower temperature for more consistent reasoning\n",
    "    max_tokens=2048,  # Explicit token limit\n",
    ")\n",
    "\n",
    "print(f\"✓ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Up Tools\n",
    "\n",
    "Tools are the functions/APIs that the agent can use to interact with the world.\n",
    "We'll use the Tavily search tool to enable web searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tools configured: ['tavily_search']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitish/Documents/github/Langchain/tools/tavily_search.py:92: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  tool = TavilySearchResults(\n"
     ]
    }
   ],
   "source": [
    "# Get the Tavily search tool\n",
    "search_tool = get_tavily_search_tool(\n",
    "    max_results=5,\n",
    "    search_depth=\"advanced\"\n",
    ")\n",
    "\n",
    "# Create tools list\n",
    "tools = [search_tool]\n",
    "\n",
    "print(f\"✓ Tools configured: {[tool.name for tool in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the ReAct Agent\n",
    "\n",
    "The ReAct agent uses a specific prompt template that guides the LLM to:\n",
    "- Think through problems step-by-step\n",
    "- Decide which tools to use\n",
    "- Process observations from tool outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Middleware & Deployment Options\n",
    "To give the ReAct agent general-purpose muscles, we can layer in LangChain's [built-in middleware](https://docs.langchain.com/oss/python/langchain/middleware/built-in) plus production backends from [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview), [LangServe](https://python.langchain.com/docs/guides/langserve/), and [LangSmith](https://docs.langchain.com/langsmith/observability):\n",
    "- **Summarization Middleware** keeps long chats within the context window by compressing older turns.\n",
    "- **Tool Retry Middleware** automatically retries flaky tools (great for web search APIs).\n",
    "- **Model Fallback Middleware** provides resilience by routing to backup models when the primary fails.\n",
    "- **LLM Tool Selector Middleware** lets an auxiliary LLM pick the most relevant tools per turn, which is ideal once you expose many utilities.\n",
    "- **LangGraph checkpointer** (InMemorySaver here) unlocks human-in-the-loop pauses and durable execution.\n",
    "- **LangSmith tracing** gives observability/tracing with zero code once `LANGCHAIN_TRACING_V2=true`.\n",
    "- **LangServe + FastAPI** exposes the agent as a REST endpoint so other services can call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SummarizationMiddleware.__init__() got an unexpected keyword argument 'trigger'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Configure middleware and backend scaffolding\u001b[39;00m\n\u001b[32m      2\u001b[39m checkpointer = InMemorySaver()\n\u001b[32m      3\u001b[39m middleware_stack = [\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mSummarizationMiddleware\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrigger\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m      9\u001b[39m     ToolRetryMiddleware(max_retries=\u001b[32m2\u001b[39m, backoff_factor=\u001b[32m1.5\u001b[39m, initial_delay=\u001b[32m1.0\u001b[39m),\n\u001b[32m     10\u001b[39m     ModelFallbackMiddleware(\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclaude-3-5-sonnet-20241022\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     ),\n\u001b[32m     14\u001b[39m     LLMToolSelectorMiddleware(\n\u001b[32m     15\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m         max_tools=\u001b[32m3\u001b[39m,\n\u001b[32m     17\u001b[39m         always_include=[tools[\u001b[32m0\u001b[39m].name] \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[32m     18\u001b[39m     ),\n\u001b[32m     19\u001b[39m ]\n\u001b[32m     21\u001b[39m backend_plan = {\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlanggraph\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mInMemorySaver checkpoint enables pauses + resume for Human-in-the-loop flows.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlangsmith\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mSet LANGCHAIN_TRACING_V2=true to stream traces, metrics, and alerts.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlangserve\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mOptional FastAPI route to expose the agent via REST/WebSocket.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m }\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Middleware stack ready:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: SummarizationMiddleware.__init__() got an unexpected keyword argument 'trigger'"
     ]
    }
   ],
   "source": [
    "# Configure middleware and backend scaffolding (LangChain 1.0.5+)\n",
    "checkpointer = InMemorySaver()\n",
    "middleware_stack = [\n",
    "    SummarizationMiddleware(\n",
    "        model=\"gpt-5-nano\",\n",
    "        trigger={\"tokens\": 4000},  # Trigger after 4K tokens\n",
    "        keep={\"messages\": 20},  # Keep last 20 messages\n",
    "    ),\n",
    "    ToolRetryMiddleware(\n",
    "        max_retries=2,\n",
    "        backoff_factor=1.5,\n",
    "        initial_delay=1.0,\n",
    "    ),\n",
    "    ModelFallbackMiddleware(\n",
    "        \"gpt-5-nano\",\n",
    "        \"gpt-4.1-mini\",\n",
    "    ),\n",
    "    LLMToolSelectorMiddleware(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        max_tools=3,\n",
    "        always_include=[tools[0].name] if tools else [],\n",
    "    ),\n",
    "]\n",
    "\n",
    "backend_plan = {\n",
    "    \"langgraph\": \"InMemorySaver checkpoint enables pauses + resume for Human-in-the-loop flows.\",\n",
    "    \"langsmith\": \"Set LANGCHAIN_TRACING_V2=true to stream traces, metrics, and alerts.\",\n",
    "    \"langserve\": \"Optional FastAPI route to expose the agent via REST/WebSocket.\",\n",
    "}\n",
    "\n",
    "print(\"✓ Middleware stack ready (LangChain 1.0.5+):\")\n",
    "for middleware in middleware_stack:\n",
    "    print(f\"  - {middleware.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5b. Checking Version Compatibility\n",
    "Verify your LangChain version is 1.0.5 or later for full compatibility with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ReAct agent created (v1.0+ syntax)!\n"
     ]
    }
   ],
   "source": [
    "# Create the ReAct agent with middleware (LangChain 1.0.5+ syntax)\n",
    "# create_agent handles ReAct internally; no hub.pull() needed\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    middleware=middleware_stack,\n",
    "    checkpointer=checkpointer,\n",
    "    system_prompt=\"You are a helpful research assistant. Use available tools to find accurate information and provide comprehensive answers.\",\n",
    ")\n",
    "\n",
    "print(\"✓ ReAct agent created (LangChain 1.0.5+ syntax)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Plug into LangSmith & LangServe\n",
    "The [LangSmith observability stack](https://docs.langchain.com/langsmith/observability) streams every model/tool call when `LANGCHAIN_TRACING_V2=true`, and [LangServe](https://python.langchain.com/docs/guides/langserve/) wraps our agent executor in FastAPI routes so other services can invoke it over REST/WebSocket. Use this optional cell to verify tracing and expose an API when deploying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: wire up LangSmith tracing & LangServe deployment (LangChain 1.0.5+)\n",
    "langsmith_enabled = os.environ.get(\"LANGCHAIN_TRACING_V2\", \"\").lower() == \"true\"\n",
    "if langsmith_enabled:\n",
    "    print(\"✓ LangSmith tracing is ON — runs will stream to your project dashboard.\")\n",
    "else:\n",
    "    print(\"ℹ Set LANGCHAIN_TRACING_V2=true (and LANGCHAIN_API_KEY/LANGCHAIN_PROJECT) to enable LangSmith traces.\")\n",
    "\n",
    "# Optional: LangServe FastAPI deployment\n",
    "if FastAPI and add_routes:\n",
    "    app = FastAPI(title=\"ReAct Agent Service\", version=\"0.1.0\")\n",
    "    # In 1.0.5+, agents are Runnables — pass directly to add_routes\n",
    "    add_routes(app, agent, path=\"/react-agent\")\n",
    "    print(\"✓ LangServe route mounted at /react-agent (launch with `uvicorn app:app --reload`).\")\n",
    "else:\n",
    "    print(\"ℹ Install `fastapi` and `langserve` to expose this agent over REST/WebSocket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example 1: Simple Information Retrieval\n",
    "\n",
    "Let's ask the agent to find current information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent with a query using v1.0 syntax\n",
    "# Invoke with messages dict (not 'input' dict)\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What are the latest developments in artificial intelligence in 2024?\"}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANSWER:\")  \n",
    "print(\"=\"*80)\n",
    "# In v1.0+, the last message contains the agent's response\n",
    "if result[\"messages\"]:\n",
    "    last_msg = result[\"messages\"][-1]\n",
    "    print(last_msg.content if hasattr(last_msg, 'content') else last_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example 2: Multi-Step Reasoning\n",
    "\n",
    "Now let's try a more complex query that requires multiple reasoning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex query requiring research and synthesis\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Who is the current CEO of OpenAI and what is their background?\"}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "if result[\"messages\"]:\n",
    "    last_msg = result[\"messages\"][-1]\n",
    "    print(last_msg.content if hasattr(last_msg, 'content') else last_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example 3: Comparative Analysis\n",
    "\n",
    "The agent can perform comparative analysis by gathering multiple pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative query\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Compare the main features of GPT-4 and Claude 3. What are the key differences?\"}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "if result[\"messages\"]:\n",
    "    last_msg = result[\"messages\"][-1]\n",
    "    print(last_msg.content if hasattr(last_msg, 'content') else last_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Understanding the ReAct Loop\n",
    "\n",
    "The ReAct agent follows this pattern:\n",
    "\n",
    "```\n",
    "Thought: I need to find information about X\n",
    "Action: tavily_search\n",
    "Action Input: \"X latest information\"\n",
    "Observation: [Search results]\n",
    "Thought: Based on the results, I can answer...\n",
    "Final Answer: [Agent's response]\n",
    "```\n",
    "\n",
    "This loop continues until the agent determines it has enough information to provide a final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Query\n",
    "\n",
    "Try your own query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom query here\n",
    "custom_query = \"What is LangChain and how is it used in AI applications?\"\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": custom_query}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL ANSWER:\")\n",
    "print(\"=\"*80)\n",
    "if result[\"messages\"]:\n",
    "    last_msg = result[\"messages\"][-1]\n",
    "    print(last_msg.content if hasattr(last_msg, 'content') else last_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "- ✓ How to set up a ReAct agent with LangChain\n",
    "- ✓ How to integrate tools (Tavily search) with agents\n",
    "- ✓ How the ReAct reasoning loop works\n",
    "- ✓ How to use agents for information retrieval and analysis\n",
    "\n",
    "## Next Steps\n",
    "- Explore adding more tools to the agent\n",
    "- Try different LLM models\n",
    "- Experiment with custom prompts\n",
    "- Check out `01.deep_agents_basic.ipynb` for more advanced agent patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
